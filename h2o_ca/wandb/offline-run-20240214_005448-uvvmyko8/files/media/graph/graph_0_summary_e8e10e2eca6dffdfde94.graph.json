{"format": "torch", "nodes": [{"name": "mlp_smpl_pose", "id": 140290949692432, "class_name": "MLP(\n  (layers): Sequential(\n    (0): Linear(in_features=72, out_features=128, bias=True)\n  )\n)", "parameters": [["layers.0.weight", [128, 72]], ["layers.0.bias", [128]]], "output_shape": [[12, 128]], "num_parameters": [9216, 128]}, {"name": "mlp_obj_pose", "id": 140290815637344, "class_name": "MLP(\n  (layers): Sequential(\n    (0): Linear(in_features=3, out_features=128, bias=True)\n  )\n)", "parameters": [["layers.0.weight", [128, 3]], ["layers.0.bias", [128]]], "output_shape": [[12, 128]], "num_parameters": [384, 128]}, {"name": "mlp_smpl_joints", "id": 140290815636384, "class_name": "MLP(\n  (layers): Sequential(\n    (0): Linear(in_features=72, out_features=128, bias=True)\n  )\n)", "parameters": [["layers.0.weight", [128, 72]], ["layers.0.bias", [128]]], "output_shape": [[12, 128]], "num_parameters": [9216, 128]}, {"name": "mlp_obj_trans", "id": 140290958791968, "class_name": "MLP(\n  (layers): Sequential(\n    (0): Linear(in_features=3, out_features=128, bias=True)\n  )\n)", "parameters": [["layers.0.weight", [128, 3]], ["layers.0.bias", [128]]], "output_shape": [[12, 128]], "num_parameters": [384, 128]}, {"name": "transformer_model_pose", "id": 140290958792640, "class_name": "Transformer(\n  (encoder): TransformerEncoder(\n    (layers): ModuleList(\n      (0-1): 2 x TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n        )\n        (linear1): Linear(in_features=128, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.05, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=128, bias=True)\n        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.05, inplace=False)\n        (dropout2): Dropout(p=0.05, inplace=False)\n      )\n    )\n    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n  )\n  (decoder): TransformerDecoder(\n    (layers): ModuleList(\n      (0): TransformerDecoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n        )\n        (multihead_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n        )\n        (linear1): Linear(in_features=128, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.05, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=128, bias=True)\n        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n        (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.05, inplace=False)\n        (dropout2): Dropout(p=0.05, inplace=False)\n        (dropout3): Dropout(p=0.05, inplace=False)\n      )\n    )\n    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n  )\n)", "parameters": [["encoder.layers.0.self_attn.in_proj_weight", [384, 128]], ["encoder.layers.0.self_attn.in_proj_bias", [384]], ["encoder.layers.0.self_attn.out_proj.weight", [128, 128]], ["encoder.layers.0.self_attn.out_proj.bias", [128]], ["encoder.layers.0.linear1.weight", [2048, 128]], ["encoder.layers.0.linear1.bias", [2048]], ["encoder.layers.0.linear2.weight", [128, 2048]], ["encoder.layers.0.linear2.bias", [128]], ["encoder.layers.0.norm1.weight", [128]], ["encoder.layers.0.norm1.bias", [128]], ["encoder.layers.0.norm2.weight", [128]], ["encoder.layers.0.norm2.bias", [128]], ["encoder.layers.1.self_attn.in_proj_weight", [384, 128]], ["encoder.layers.1.self_attn.in_proj_bias", [384]], ["encoder.layers.1.self_attn.out_proj.weight", [128, 128]], ["encoder.layers.1.self_attn.out_proj.bias", [128]], ["encoder.layers.1.linear1.weight", [2048, 128]], ["encoder.layers.1.linear1.bias", [2048]], ["encoder.layers.1.linear2.weight", [128, 2048]], ["encoder.layers.1.linear2.bias", [128]], ["encoder.layers.1.norm1.weight", [128]], ["encoder.layers.1.norm1.bias", [128]], ["encoder.layers.1.norm2.weight", [128]], ["encoder.layers.1.norm2.bias", [128]], ["encoder.norm.weight", [128]], ["encoder.norm.bias", [128]], ["decoder.layers.0.self_attn.in_proj_weight", [384, 128]], ["decoder.layers.0.self_attn.in_proj_bias", [384]], ["decoder.layers.0.self_attn.out_proj.weight", [128, 128]], ["decoder.layers.0.self_attn.out_proj.bias", [128]], ["decoder.layers.0.multihead_attn.in_proj_weight", [384, 128]], ["decoder.layers.0.multihead_attn.in_proj_bias", [384]], ["decoder.layers.0.multihead_attn.out_proj.weight", [128, 128]], ["decoder.layers.0.multihead_attn.out_proj.bias", [128]], ["decoder.layers.0.linear1.weight", [2048, 128]], ["decoder.layers.0.linear1.bias", [2048]], ["decoder.layers.0.linear2.weight", [128, 2048]], ["decoder.layers.0.linear2.bias", [128]], ["decoder.layers.0.norm1.weight", [128]], ["decoder.layers.0.norm1.bias", [128]], ["decoder.layers.0.norm2.weight", [128]], ["decoder.layers.0.norm2.bias", [128]], ["decoder.layers.0.norm3.weight", [128]], ["decoder.layers.0.norm3.bias", [128]], ["decoder.norm.weight", [128]], ["decoder.norm.bias", [128]]], "output_shape": [[12, 1, 128]], "num_parameters": [49152, 384, 16384, 128, 262144, 2048, 262144, 128, 128, 128, 128, 128, 49152, 384, 16384, 128, 262144, 2048, 262144, 128, 128, 128, 128, 128, 128, 128, 49152, 384, 16384, 128, 49152, 384, 16384, 128, 262144, 2048, 262144, 128, 128, 128, 128, 128, 128, 128, 128, 128]}, {"name": "transformer_model_trans", "id": 140290958792112, "class_name": "Transformer(\n  (encoder): TransformerEncoder(\n    (layers): ModuleList(\n      (0-1): 2 x TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n        )\n        (linear1): Linear(in_features=128, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.05, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=128, bias=True)\n        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.05, inplace=False)\n        (dropout2): Dropout(p=0.05, inplace=False)\n      )\n    )\n    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n  )\n  (decoder): TransformerDecoder(\n    (layers): ModuleList(\n      (0): TransformerDecoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n        )\n        (multihead_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n        )\n        (linear1): Linear(in_features=128, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.05, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=128, bias=True)\n        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n        (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.05, inplace=False)\n        (dropout2): Dropout(p=0.05, inplace=False)\n        (dropout3): Dropout(p=0.05, inplace=False)\n      )\n    )\n    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n  )\n)", "parameters": [["encoder.layers.0.self_attn.in_proj_weight", [384, 128]], ["encoder.layers.0.self_attn.in_proj_bias", [384]], ["encoder.layers.0.self_attn.out_proj.weight", [128, 128]], ["encoder.layers.0.self_attn.out_proj.bias", [128]], ["encoder.layers.0.linear1.weight", [2048, 128]], ["encoder.layers.0.linear1.bias", [2048]], ["encoder.layers.0.linear2.weight", [128, 2048]], ["encoder.layers.0.linear2.bias", [128]], ["encoder.layers.0.norm1.weight", [128]], ["encoder.layers.0.norm1.bias", [128]], ["encoder.layers.0.norm2.weight", [128]], ["encoder.layers.0.norm2.bias", [128]], ["encoder.layers.1.self_attn.in_proj_weight", [384, 128]], ["encoder.layers.1.self_attn.in_proj_bias", [384]], ["encoder.layers.1.self_attn.out_proj.weight", [128, 128]], ["encoder.layers.1.self_attn.out_proj.bias", [128]], ["encoder.layers.1.linear1.weight", [2048, 128]], ["encoder.layers.1.linear1.bias", [2048]], ["encoder.layers.1.linear2.weight", [128, 2048]], ["encoder.layers.1.linear2.bias", [128]], ["encoder.layers.1.norm1.weight", [128]], ["encoder.layers.1.norm1.bias", [128]], ["encoder.layers.1.norm2.weight", [128]], ["encoder.layers.1.norm2.bias", [128]], ["encoder.norm.weight", [128]], ["encoder.norm.bias", [128]], ["decoder.layers.0.self_attn.in_proj_weight", [384, 128]], ["decoder.layers.0.self_attn.in_proj_bias", [384]], ["decoder.layers.0.self_attn.out_proj.weight", [128, 128]], ["decoder.layers.0.self_attn.out_proj.bias", [128]], ["decoder.layers.0.multihead_attn.in_proj_weight", [384, 128]], ["decoder.layers.0.multihead_attn.in_proj_bias", [384]], ["decoder.layers.0.multihead_attn.out_proj.weight", [128, 128]], ["decoder.layers.0.multihead_attn.out_proj.bias", [128]], ["decoder.layers.0.linear1.weight", [2048, 128]], ["decoder.layers.0.linear1.bias", [2048]], ["decoder.layers.0.linear2.weight", [128, 2048]], ["decoder.layers.0.linear2.bias", [128]], ["decoder.layers.0.norm1.weight", [128]], ["decoder.layers.0.norm1.bias", [128]], ["decoder.layers.0.norm2.weight", [128]], ["decoder.layers.0.norm2.bias", [128]], ["decoder.layers.0.norm3.weight", [128]], ["decoder.layers.0.norm3.bias", [128]], ["decoder.norm.weight", [128]], ["decoder.norm.bias", [128]]], "output_shape": [[12, 1, 128]], "num_parameters": [49152, 384, 16384, 128, 262144, 2048, 262144, 128, 128, 128, 128, 128, 49152, 384, 16384, 128, 262144, 2048, 262144, 128, 128, 128, 128, 128, 128, 128, 49152, 384, 16384, 128, 49152, 384, 16384, 128, 262144, 2048, 262144, 128, 128, 128, 128, 128, 128, 128, 128, 128]}, {"name": "mlp_output_pose", "id": 140294061398864, "class_name": "MLP(\n  (layers): Sequential(\n    (0): Linear(in_features=128, out_features=3, bias=True)\n  )\n)", "parameters": [["layers.0.weight", [3, 128]], ["layers.0.bias", [3]]], "output_shape": [[1, 12, 3]], "num_parameters": [384, 3]}, {"name": "mlp_output_trans", "id": 140294085001616, "class_name": "MLP(\n  (layers): Sequential(\n    (0): Linear(in_features=128, out_features=3, bias=True)\n  )\n)", "parameters": [["layers.0.weight", [3, 128]], ["layers.0.bias", [3]]], "output_shape": [[1, 12, 3]], "num_parameters": [384, 3]}], "edges": []}